# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Luagents is a ReAct (Reasoning and Acting) agent implementation in Elixir that uses Lua code for reasoning, inspired by HuggingFace's smolagents. The system creates agents that can think step-by-step by generating and executing Lua code to solve problems.

## Common Commands

### Development
```bash
mix deps.get           # Install dependencies
mix compile            # Compile the project
mix test               # Run all tests
mix test test/path/to/specific_test.exs  # Run specific test file
mix dialyzer           # Run Dialyzer type checking
mix credo              # Run code style/quality checks
```

### Examples
```bash
mix run examples/minimal.exs              # Simple calculation example
mix run examples/anthropic_example.exs    # Anthropic Claude example
mix run examples/react_demo.exs           # ReAct pattern demonstration
mix run examples/improved_example.exs     # Advanced multi-scenario example
```

### LLM Provider Setup
```bash
# For Ollama (local)
brew install ollama
ollama serve
ollama pull llama3.2

# For Anthropic (cloud)
export ANTHROPIC_API_KEY="your-key"
```

## Architecture Overview

### Core ReAct Loop
The system implements a ReAct pattern where agents:
1. Receive a task from the user
2. Use an LLM to generate Lua code for step-by-step reasoning
3. Execute the Lua code which can call tools and use special functions
4. Continue iterating until reaching a final answer

### Key Modules

**`Luagents.Agent`** - Main agent orchestrator that manages the ReAct loop, coordinates between LLM, Lua execution, and memory management. Contains the core `run_loop/2` function that drives the reasoning process.

**`Luagents.LuaEngine`** - Executes Lua code generated by LLMs. Manages Lua state persistence, tool injection, and special functions (`thought()`, `observation()`, `final_answer()`). Handles both successful execution and error recovery.

**`Luagents.LLM`** - Factory module for creating LLM instances. Supports pluggable providers (Anthropic, Ollama) through a behaviour pattern. Each provider implements `new/1` and `generate/2`.

**`Luagents.Prompts`** - Contains the system prompt that instructs LLMs how to generate proper Lua code for ReAct reasoning. Includes examples and rules for tool usage.

**`Luagents.Tool`** - Defines tool interface and built-in tools (`add`, `multiply`, `concat`, `search`). Tools are Elixir functions that can be called from Lua code.

**`Luagents.Memory`** - Manages conversation history between user, assistant, and system messages throughout the ReAct loop iterations.

### LLM Provider Architecture

**Provider Pattern**: Each LLM provider implements the `Luagents.LLM.Behaviour`:
- `Luagents.LLM.Anthropic` - Uses Anthropix library for Claude API
- `Luagents.LLM.Ollama` - Uses Ollama library for local models
- `Luagents.LLM.Utils` - Shared utilities for code extraction and error formatting

### Lua Integration

The system uses the `lua` package to execute generated code. Key aspects:
- **State Persistence**: Lua variables and functions persist across execution cycles
- **Tool Injection**: Elixir functions are exposed as Lua functions at runtime  
- **Special Functions**: `thought()`, `observation()`, `final_answer()` are injected for ReAct pattern
- **Print Buffer**: Captures Lua print output for debugging and observation

### Tool System

Tools are the bridge between Lua reasoning and Elixir capabilities:
- Defined as structs with name, description, parameters, and implementation function
- Built-in tools provide basic operations (math, string manipulation, mock search)
- Custom tools can be added by implementing the tool structure
- Tools receive arguments as lists and return `{:ok, result}` or `{:error, reason}`

## Key Configuration

- **Default LLM Provider**: Anthropic Claude (falls back to Ollama if no API key)
- **Default Iterations**: 10 maximum iterations per task
- **Lua State Management**: State persists between iterations but resets per task
- **Error Handling**: Both Lua compilation errors and runtime errors are caught and reported

## Testing Patterns

- **Provider Testing**: Tests verify both Anthropic and Ollama can be instantiated
- **Behaviour Testing**: Tests ensure consistent interfaces across providers
- **Integration Testing**: Tests verify full ReAct loops with mock LLMs
- **Doctest Coverage**: Core API functions include doctest examples

## Special Considerations

- The system expects LLMs to generate Lua code blocks (marked with ````lua` or generic `````)
- Tool function signatures must match exactly what the LLM generates
- Print buffer management is critical for observation() function output
- API key management supports multiple methods (env vars, parameters, app config)
- Error recovery allows agents to continue reasoning after tool failures